{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение. Семинар и домашнее задание 1. Обучение полносвязной нейронной сети на numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "[__ Конспект с выводом формул__](https://github.com/nadiinchi/dl_labs/blob/master/nn_gradients.pdf)\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        ### your code here\n",
    "        self.input = input\n",
    "        return input * (input > 0)\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        return grad_output * (self.input > 0), []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        self.input = input\n",
    "        return input.dot(self.weights) + self.biases\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        d_W = self.input.T.dot(grad_output)\n",
    "        d_b = grad_output.sum(axis=0)\n",
    "        d_ = grad_output.dot(self.weights.T)\n",
    "        return d_, np.r_[d_W.ravel(), d_b].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "### your code here\n",
    "relu = ReLU()\n",
    "relu.forward(points)\n",
    "grads = relu.backward(np.ones((10, 12)))[0]\n",
    "relu = ReLU()\n",
    "f = lambda x: relu.forward(x).sum()\n",
    "numeric_grads = eval_numerical_gradient(f, points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        self.input = input\n",
    "        if input.ndim == 1:\n",
    "            self.res = input - logsumexp(input)\n",
    "            return self.res\n",
    "        self.res = input - logsumexp(input, axis=1)[:, np.newaxis]\n",
    "        return self.res\n",
    "    \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        res = grad_output + np.exp(self.res)\n",
    "        return res, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    return -np.sum(activations * target)\n",
    "    \n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num-classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    return -target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*10).reshape([10, 10])\n",
    "target = np.arange(10)\n",
    "### your code here\n",
    "target = (np.diag(target + 1) != 0).astype('int')\n",
    "sfm = Softmax()\n",
    "lsf = sfm.forward(points)\n",
    "grads = sfm.backward(grad_crossentropy(lsf, target))[0]\n",
    "f = lambda x: crossentropy(sfm.forward(x), target)\n",
    "numeric_grads = eval_numerical_gradient(f, points)\n",
    "\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сборка и обучение нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    ### your code here\n",
    "    cur_input = X.copy()\n",
    "    for layer in network:\n",
    "        cur_input = layer.forward(cur_input)\n",
    "    return cur_input.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    In general, the optimization problems are of the form::\n",
      "    \n",
      "        minimize f(x) subject to\n",
      "    \n",
      "        g_i(x) >= 0,  i = 1,...,m\n",
      "        h_j(x)  = 0,  j = 1,...,p\n",
      "    \n",
      "    where x is a vector of one or more variables.\n",
      "    ``g_i(x)`` are the inequality constraints.\n",
      "    ``h_j(x)`` are the equality constrains.\n",
      "    \n",
      "    Optionally, the lower and upper bounds for each element in x can also be\n",
      "    specified using the `bounds` argument.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized. Must be in the form\n",
      "        ``f(x, *args)``. The optimizing argument, ``x``, is a 1-D array\n",
      "        of points, and ``args`` is a tuple of any additional fixed parameters\n",
      "        needed to completely specify the function.\n",
      "    x0 : ndarray\n",
      "        Initial guess. ``len(x0)`` is the dimensionality of the minimization\n",
      "        problem.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (Jacobian, Hessian).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : bool or callable, optional\n",
      "        Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-region-exact.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the\n",
      "        gradient will be estimated numerically.\n",
      "        `jac` can also be a callable returning the gradient of the\n",
      "        objective. In this case, it must accept the same arguments as `fun`.\n",
      "    hess, hessp : callable, optional\n",
      "        Hessian (matrix of second-order derivatives) of objective function or\n",
      "        Hessian of objective function times an arbitrary vector p.  Only for\n",
      "        Newton-CG, dogleg, trust-ncg, trust-krylov, trust-region-exact.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  If neither `hess` nor\n",
      "        `hessp` is provided, then the Hessian product will be approximated\n",
      "        using finite differences on `jac`. `hessp` must compute the Hessian\n",
      "        times an arbitrary vector.\n",
      "    bounds : sequence, optional\n",
      "        Bounds for variables (only for L-BFGS-B, TNC and SLSQP).\n",
      "        ``(min, max)`` pairs for each element in ``x``, defining\n",
      "        the bounds on that parameter. Use None for one of ``min`` or\n",
      "        ``max`` when there is no bound in that direction.\n",
      "    constraints : dict or sequence of dict, optional\n",
      "        Constraints definition (only for COBYLA and SLSQP).\n",
      "        Each constraint is defined in a dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "        current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iteraction\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", https://arxiv.org/abs/1611.04718\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    ### your code here\n",
    "    set_weights(weights, network)\n",
    "    cur_input = X.copy()\n",
    "    for layer in network:\n",
    "        cur_input = layer.forward(cur_input)\n",
    "    Q = crossentropy(cur_input, y)\n",
    "    \n",
    "    cur_grad = [grad_crossentropy(cur_input, y)]\n",
    "    for layer in reversed(network):\n",
    "        cur_grad.append(layer.backward(cur_grad[-1])[0])\n",
    "    g = []\n",
    "    for i, layer in enumerate(network):\n",
    "        g += layer.backward(cur_grad[len(network) - i - 1])[1]\n",
    "    return Q, np.array(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tmp = np.zeros((y_train.size, 10))\n",
    "for i, y_ in enumerate(y_train_tmp):\n",
    "    y_[y_train[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train_tmp], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00476812,  0.00679067,  0.01214229, ..., -1.00809832,\n",
       "       -2.15917406, -0.39215221])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test. Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(network, X, y):\n",
    "    return (predict(network, X) == \n",
    "            y).sum() / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train:  1.0\n",
      "Accuracy in test:  0.9577777777777777\n"
     ]
    }
   ],
   "source": [
    "### your code here\n",
    "set_weights(res[\"x\"], network)\n",
    "print(\"Accuracy in train: \", get_acc(network, X_train, y_train))\n",
    "print(\"Accuracy in test: \", get_acc(network, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, verbose=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.verbose = verbose\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        ### your code here\n",
    "        set_weights(weights, network)\n",
    "        self.train_acc.append(get_acc(network, X_train, y_train))\n",
    "        self.test_acc.append(get_acc(network, X_test, y_test))\n",
    "        if self.verbose:\n",
    "            print(\"Accuracy in train set: {}\\nAccuracy in test set: {}\\n\".format(self.train_acc[-1],\n",
    "                                                                                 self.test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train set: 0.10616184112843356\n",
      "Accuracy in test set: 0.08444444444444445\n",
      "\n",
      "Accuracy in train set: 0.10244988864142539\n",
      "Accuracy in test set: 0.1\n",
      "\n",
      "Accuracy in train set: 0.16703786191536749\n",
      "Accuracy in test set: 0.18888888888888888\n",
      "\n",
      "Accuracy in train set: 0.12397921306607275\n",
      "Accuracy in test set: 0.13333333333333333\n",
      "\n",
      "Accuracy in train set: 0.2873051224944321\n",
      "Accuracy in test set: 0.29333333333333333\n",
      "\n",
      "Accuracy in train set: 0.31997030438010393\n",
      "Accuracy in test set: 0.33555555555555555\n",
      "\n",
      "Accuracy in train set: 0.40757238307349664\n",
      "Accuracy in test set: 0.4111111111111111\n",
      "\n",
      "Accuracy in train set: 0.5352635486265775\n",
      "Accuracy in test set: 0.5355555555555556\n",
      "\n",
      "Accuracy in train set: 0.5523385300668151\n",
      "Accuracy in test set: 0.56\n",
      "\n",
      "Accuracy in train set: 0.6696362286562731\n",
      "Accuracy in test set: 0.6711111111111111\n",
      "\n",
      "Accuracy in train set: 0.7008166295471417\n",
      "Accuracy in test set: 0.6733333333333333\n",
      "\n",
      "Accuracy in train set: 0.7104677060133631\n",
      "Accuracy in test set: 0.6977777777777778\n",
      "\n",
      "Accuracy in train set: 0.7223459539717891\n",
      "Accuracy in test set: 0.7\n",
      "\n",
      "Accuracy in train set: 0.7661469933184856\n",
      "Accuracy in test set: 0.7333333333333333\n",
      "\n",
      "Accuracy in train set: 0.7728285077951003\n",
      "Accuracy in test set: 0.7488888888888889\n",
      "\n",
      "Accuracy in train set: 0.8092056421677802\n",
      "Accuracy in test set: 0.78\n",
      "\n",
      "Accuracy in train set: 0.8210838901262064\n",
      "Accuracy in test set: 0.7933333333333333\n",
      "\n",
      "Accuracy in train set: 0.844097995545657\n",
      "Accuracy in test set: 0.8222222222222222\n",
      "\n",
      "Accuracy in train set: 0.8470675575352635\n",
      "Accuracy in test set: 0.8355555555555556\n",
      "\n",
      "Accuracy in train set: 0.8611729769858946\n",
      "Accuracy in test set: 0.8266666666666667\n",
      "\n",
      "Accuracy in train set: 0.8663697104677061\n",
      "Accuracy in test set: 0.8288888888888889\n",
      "\n",
      "Accuracy in train set: 0.8812175204157386\n",
      "Accuracy in test set: 0.8422222222222222\n",
      "\n",
      "Accuracy in train set: 0.8878990348923533\n",
      "Accuracy in test set: 0.8622222222222222\n",
      "\n",
      "Accuracy in train set: 0.888641425389755\n",
      "Accuracy in test set: 0.8711111111111111\n",
      "\n",
      "Accuracy in train set: 0.8878990348923533\n",
      "Accuracy in test set: 0.8777777777777778\n",
      "\n",
      "Accuracy in train set: 0.8982925018559762\n",
      "Accuracy in test set: 0.8933333333333333\n",
      "\n",
      "Accuracy in train set: 0.9049740163325909\n",
      "Accuracy in test set: 0.9\n",
      "\n",
      "Accuracy in train set: 0.9146250927988122\n",
      "Accuracy in test set: 0.8977777777777778\n",
      "\n",
      "Accuracy in train set: 0.9168522642910171\n",
      "Accuracy in test set: 0.9022222222222223\n",
      "\n",
      "Accuracy in train set: 0.9190794357832219\n",
      "Accuracy in test set: 0.9133333333333333\n",
      "\n",
      "Accuracy in train set: 0.9257609502598366\n",
      "Accuracy in test set: 0.9133333333333333\n",
      "\n",
      "Accuracy in train set: 0.9361544172234595\n",
      "Accuracy in test set: 0.9177777777777778\n",
      "\n",
      "Accuracy in train set: 0.9420935412026726\n",
      "Accuracy in test set: 0.92\n",
      "\n",
      "Accuracy in train set: 0.9435783221974758\n",
      "Accuracy in test set: 0.9333333333333333\n",
      "\n",
      "Accuracy in train set: 0.9450631031922792\n",
      "Accuracy in test set: 0.94\n",
      "\n",
      "Accuracy in train set: 0.9517446176688938\n",
      "Accuracy in test set: 0.9311111111111111\n",
      "\n",
      "Accuracy in train set: 0.9547141796585004\n",
      "Accuracy in test set: 0.9311111111111111\n",
      "\n",
      "Accuracy in train set: 0.9651076466221232\n",
      "Accuracy in test set: 0.9355555555555556\n",
      "\n",
      "Accuracy in train set: 0.9680772086117297\n",
      "Accuracy in test set: 0.9355555555555556\n",
      "\n",
      "Accuracy in train set: 0.9680772086117297\n",
      "Accuracy in test set: 0.9444444444444444\n",
      "\n",
      "Accuracy in train set: 0.9740163325909429\n",
      "Accuracy in test set: 0.9333333333333333\n",
      "\n",
      "Accuracy in train set: 0.9740163325909429\n",
      "Accuracy in test set: 0.9377777777777778\n",
      "\n",
      "Accuracy in train set: 0.9747587230883444\n",
      "Accuracy in test set: 0.9355555555555556\n",
      "\n",
      "Accuracy in train set: 0.9792130660727543\n",
      "Accuracy in test set: 0.9333333333333333\n",
      "\n",
      "Accuracy in train set: 0.9814402375649591\n",
      "Accuracy in test set: 0.94\n",
      "\n",
      "Accuracy in train set: 0.9844097995545658\n",
      "Accuracy in test set: 0.94\n",
      "\n",
      "Accuracy in train set: 0.9836674090571641\n",
      "Accuracy in test set: 0.9488888888888889\n",
      "\n",
      "Accuracy in train set: 0.9888641425389755\n",
      "Accuracy in test set: 0.9488888888888889\n",
      "\n",
      "Accuracy in train set: 0.9844097995545658\n",
      "Accuracy in test set: 0.9444444444444444\n",
      "\n",
      "Accuracy in train set: 0.9888641425389755\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 0.9896065330363771\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 0.9910913140311804\n",
      "Accuracy in test set: 0.9444444444444444\n",
      "\n",
      "Accuracy in train set: 0.9903489235337788\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 0.9925760950259837\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 0.9948032665181886\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 0.9962880475129918\n",
      "Accuracy in test set: 0.9422222222222222\n",
      "\n",
      "Accuracy in train set: 0.9985152190051967\n",
      "Accuracy in test set: 0.9422222222222222\n",
      "\n",
      "Accuracy in train set: 0.9985152190051967\n",
      "Accuracy in test set: 0.9422222222222222\n",
      "\n",
      "Accuracy in train set: 0.9977728285077951\n",
      "Accuracy in test set: 0.9422222222222222\n",
      "\n",
      "Accuracy in train set: 0.9985152190051967\n",
      "Accuracy in test set: 0.9422222222222222\n",
      "\n",
      "Accuracy in train set: 0.9985152190051967\n",
      "Accuracy in test set: 0.9444444444444444\n",
      "\n",
      "Accuracy in train set: 0.9985152190051967\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 0.9992576095025983\n",
      "Accuracy in test set: 0.9488888888888889\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9533333333333334\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9488888888888889\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9466666666666667\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9422222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9488888888888889\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9488888888888889\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9533333333333334\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9511111111111111\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9533333333333334\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9533333333333334\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.96\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9622222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9644444444444444\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9622222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9644444444444444\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9622222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9622222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9622222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9622222222222222\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.96\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9555555555555556\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n",
      "Accuracy in train set: 1.0\n",
      "Accuracy in test set: 0.9577777777777777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, verbose=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train_tmp], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2600df349e8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2600e13a358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперименты с числом слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 5))\n",
    "accs_test = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here\n",
    "hidden_layers_size = 32\n",
    "input_layer_size = X_train.shape[1]\n",
    "def get_network(n_layer):\n",
    "    if n_layer == 1:\n",
    "        network = []\n",
    "        network.append(Dense(input_layer_size, 10))\n",
    "        network.append(Softmax())\n",
    "        return network\n",
    "    network = []\n",
    "    network.append(Dense(input_layer_size, hidden_layers_size))\n",
    "    network.append(ReLU())\n",
    "    for i in range(n_layer - 2):\n",
    "        network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "    network.append(Dense(hidden_layers_size, 10))\n",
    "    network.append(Softmax())\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        network = get_network(i+1)\n",
    "        weights = get_weights(network)\n",
    "        res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "                       args=[network, X_train, y_train_tmp], # args passed to fun\n",
    "                       method=\"L-BFGS-B\", # optimization method\n",
    "                       jac=True)\n",
    "        set_weights(res[\"x\"], network)\n",
    "        accs_train[i, j] = get_acc(network, X_train, y_train)\n",
    "        accs_test[i, j] = get_acc(network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test quality in 5 runs')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2600df5d3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        network = get_network(i+1)\n",
    "        weights = get_weights(network)\n",
    "        res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "                       args=[network, X_train, y_train_tmp], # args passed to fun\n",
    "                       method=\"L-BFGS-B\", # optimization method\n",
    "                       jac=True)\n",
    "        set_weights(res[\"x\"], network)\n",
    "        accs_train[i, j] = get_acc(network, X_train, y_train)\n",
    "        accs_test[i, j] = get_acc(network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test quality in 5 runs')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2600e0a70b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "\n",
    "На проведенных экспериментах лучшее в среднем качество имеет модель с двумя полносвязанными слоями, остальные чуть хуже.\n",
    "\n",
    "При этом нельзя сказать, что с ростом числа слоев учеличивается устойчивость модели. Так, разброс на модели с двумя слоями меньше разброса на однослойной нейронной сети, а разброс на модели с тремя слоями больше. Так же стои учесть, что при разных запусках эксперимента могут получится разные результаты по устойчивости модели и по качеству на контроле. (Выводы написаны по второму примеру запуска эксперимента.)\n",
    "\n",
    "[test_acc1]: https://github.com/apidzhakova/dl_labs/blob/master/images/test_acc1.png?raw=trueg\n",
    "[test_acc2]: https://github.com/apidzhakova/dl_labs/blob/master/images/test_acc2.png?raw=trueg\n",
    "\n",
    "![Пример 1][test_acc2] ![Пример 2][test_acc1]\n",
    "Изображения демонстрируют, что при разных запусках эксперимента результаты меняются, то есть данные модели не очень устойчивы.\n",
    "\n",
    "В связи с этим нельзя сказать, что линейная модель однозначно дает качество хуже, чем нелинейная. Так, в одном из запусков качество линейной модели в среднем лучше, чем качество нелинеймой модели, в которой четыре полносвязанных уровня."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
